{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Tutorial: TensorFlow ANN Example\n",
    "\n",
    "## What is Artificial Neural Network?\n",
    "\n",
    "An Artificial Neural Network(ANN) is composed of four principal objects:\n",
    "\n",
    "- Layers: all the learning occurs in the layers. There are 3 layers 1) Input 2) Hidden and 3) Output\n",
    "- feature and label: Input data to the network(features) and output from the network (labels)\n",
    "- loss function: Metric used to estimate the performance of the learning phase\n",
    "- optimizer: Improve the learning by updating the knowledge in the network\n",
    "\n",
    "A neural network will take the input data and push them into an ensemble of layers. The network needs to evaluate its performance with a loss function. The loss function gives to the network an idea of the path it needs to take before it masters the knowledge. The network needs to improve its knowledge with the help of an optimizer.\n",
    "\n",
    "If you take a look at the figure below, you will understand the underlying mechanism.\n",
    "\n",
    "![](./NeuralNetwork1.png)\n",
    "\n",
    "The program takes some input values and pushes them into two fully connected layers. Imagine you have a math problem, the first thing you do is to read the corresponding chapter to solve the problem. You apply your new knowledge to solve the problem. There is a high chance you will not score very well. It is the same for a network. The first time it sees the data and makes a prediction, it will not match perfectly with the actual data.\n",
    "\n",
    "To improve its knowledge, the network uses an optimizer. In our analogy, an optimizer can be thought of as rereading the chapter. You gain new insights/lesson by reading again. Similarly, the network uses the optimizer, updates its knowledge, and tests its new knowledge to check how much it still needs to learn. The program will repeat this step until it makes the lowest error possible.\n",
    "\n",
    "In our math problem analogy, it means you read the textbook chapter many times until you thoroughly understand the course content. Even after reading multiple times, if you keep making an error, it means you reached the knowledge capacity with the current material. You need to use different textbook or test different method to improve your score. For a neural network, it is the same process. If the error is far from 100%, but the curve is flat, it means with the current architecture; it cannot learn anything else. The network has to be better optimized to improve the knowledge.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "* [What is Artificial Neural Network?](#what-is-artificial-neural-network)\n",
    "* [Neural Network Architecture](#neural-network-architecture)\n",
    "* [Loss function](#loss-function)\n",
    "* [Optimizer](#optimizer)\n",
    "* [Limitations of Neural Network](#limitations-of-neural-network)\n",
    "* [Network size](#network-size)\n",
    "* [Weight Regularization](#weight-regularization)\n",
    "* [Dropout](#dropout)\n",
    "* [Example Neural Network in TensorFlow](#example-neural-network-in-tensorflow)\n",
    "* [Train a neural network with TensorFlow](#train-a-neural-network-with-tensorflow)\n",
    " * [Step 1) Import the data](#step1-import-the-data)\n",
    " * [Step 2) Transform the data](#step2-transform-the-data)\n",
    " * [Step 3) Construct the tensor](#step3-construct-the-tensor)\n",
    " * [Step 4) Build the model](#step4-build-the-model)\n",
    " * [Step 5) Train and evaluate the model](#step5-train-and-evaluate-the-model)\n",
    " * [Step 6) Improve the model](#steps6-improve-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "### Layers\n",
    "\n",
    "A layer is where all the learning takes place. Inside a layer, there are an infinite amount of weights (neurons). A typical neural network is often processed by densely connected layers (also called fully connected layers). It means all the inputs are connected to the output.\n",
    "\n",
    "A typical neural network takes a vector of input and a scalar that contains the labels. The most comfortable set up is a binary classification with only two classes: 0 and 1.\n",
    "\n",
    "The network takes an input, sends it to all connected nodes and computes the signal with an **activation** function.\n",
    "\n",
    "![](./NeuralNetwork2.png)\n",
    "\n",
    "The figure above plots this idea. The first layer is the input values for the second layer, called the hidden layer, receives the weighted input from the previous layer\n",
    "\n",
    "1. The first node is the input values\n",
    "2. The neuron is decomposed into the input part and the activation function. The left part receives all the input from the previous layer. The right part is the sum of the input passes into an activation function.\n",
    "3. Output value computed from the hidden layers and used to make a prediction. For classification, it is equal to the number of class. For regression, only one value is predicted.\n",
    "\n",
    "### Activation function\n",
    "\n",
    "The activation function of a node defines the output given a set of inputs. You need an activation function to allow the network to learn non-linear pattern. A common activation function is a Relu, Rectified linear unit. The function gives a zero for all negative values.\n",
    "\n",
    "![](./NeuralNetwork3.png)\n",
    "\n",
    "The other activation functions are:\n",
    "\n",
    "- Piecewise Linear\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- Leaky Relu\n",
    "\n",
    "The critical decision to make when building a neural network is:\n",
    "\n",
    "- How many layers in the neural network\n",
    "- How many hidden units for each layer\n",
    "- Neural network with lots of layers and hidden units can learn a complex representation of the data, but it makes the network's computation very expensive.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "After you have defined the hidden layers and the activation function, you need to specify the loss function and the optimizer.\n",
    "\n",
    "For binary classification, it is common practice to use a binary cross entropy loss function. In the linear regression, you use the mean square error.\n",
    "\n",
    "The loss function is an important metric to estimate the performance of the optimizer. During the training, this metric will be minimized. You need to select this quantity carefully depending on the type of problem you are dealing with.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The loss function is a measure of the model's performance. The optimizer will help improve the weights of the network in order to decrease the loss. There are different optimizers available, but the most common one is the **Stochastic Gradient Descent**.\n",
    "\n",
    "The conventional optimizers are:\n",
    "\n",
    "- Momentum optimization,\n",
    "- Nesterov Accelerated Gradient,\n",
    "- AdaGrad,\n",
    "- Adam optimization\n",
    "\n",
    "## Limitations of Neural Network\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "A common problem with the complex neural net is the difficulties in generalizing unseen data. A neural network with lots of weights can identify specific details in the train set very well but often leads to overfitting. If the data are unbalanced within groups (i.e., not enough data available in some groups), the network will learn very well during the training but will not have the ability to generalize such pattern to never-seen-before data.\n",
    "\n",
    "There is a trade-off in machine learning between optimization and generalization.\n",
    "\n",
    "Optimizing a model requires to find the best parameters that minimize the loss of the training set.\n",
    "\n",
    "Generalization, however, tells how the model behaves for unseen data.\n",
    "\n",
    "To prevent the model from capturing specific details or unwanted patterns of the training data, you can use different techniques. The best method is to have a balanced dataset with sufficient amount of data. The art of reducing overfitting is called **regularization**. Let's review some conventional techniques.\n",
    "\n",
    "### Network size\n",
    "\n",
    "A neural network with too many layers and hidden units are known to be highly sophisticated. A straightforward way to reduce the complexity of the model is to reduce its size. There is no best practice to define the number of layers. You need to start with a small amount of layer and increases its size until you find the model overfit.\n",
    "\n",
    "### Weight Regularization\n",
    "\n",
    "A standard technique to prevent overfitting is to add constraints to the weights of the network. The constraint forces the size of the network to take only small values. The constraint is added to the loss function of the error. There are two kinds of regularization:\n",
    "\n",
    "- **L1: Lasso**: Cost is proportional to the absolute value of the weight coefficients\n",
    "\n",
    "- **L2: Ridge**: Cost is proportional to the square of the value of the weight coefficients\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is an odd but useful technique. A network with dropout means that some weights will be randomly set to zero. Imagine you have an array of weights [0.1, 1.7, 0.7, -0.9]. If the neural network has a dropout, it will become [0.1, 0, 0, -0.9] with randomly distributed 0. The parameter that controls the dropout is the dropout rate. The rate defines how many weights to be set to zeroes. Having a rate between 0.2 and 0.5 is common.\n",
    "\n",
    "## Example Neural Network in TensorFlow\n",
    "\n",
    "Let's see in action how a neural network works for a typical classification problem. There are two inputs, x1 and x2 with a random value. The output is a binary class. The objective is to classify the label based on the two features. To carry out this task, the neural network architecture is defined as following:\n",
    "\n",
    "- Two hidden layers\n",
    " - First layer has four fully connected neurons\n",
    " - Second layer has two fully connected neurons\n",
    "- The activation function is a Relu\n",
    "- Add an L2 Regularization with a learning rate of 0.003\n",
    "\n",
    "![](./NeuralNetwork4.gif)\n",
    "\n",
    "The network will optimize the weight during 180 epochs with a batch size of 10. In the video below you can see how the weights evolve over and how the network improves the classification mapping.\n",
    "\n",
    "First of all, the network assigns random values to all the weights.\n",
    "\n",
    "- With the random weights, i.e., without optimization, the output loss is 0.453. The picture below represents the network with different colors.\n",
    "- In general, the orange color represents negative values while the blue colors show the positive values.\n",
    "- The data points have the same representation; the blue ones are the positive labels and the orange one the negative labels.\n",
    "\n",
    "![](./NeuralNetwork.png)\n",
    "\n",
    "Inside the second hidden layer, the lines are colored following the sign of the weights. The orange lines assign negative weights and the blue one a positive weights\n",
    "\n",
    "As you can see, in the output mapping, the network is making quite a lot of mistake. Let's see how the network behaves after optimization.\n",
    "\n",
    "The picture below depicts the results of the optimized network. First of all, you notice the network has successfully learned how to classify the data point. You can see from the picture before; the initial weight was -0.43 while after optimization it results in a weight of -0.95.\n",
    "\n",
    "![](./NeuralNetwork7.png)\n",
    "\n",
    "The idea can be generalized for networks with more hidden layers and neurons. You can play around in the [link](http://playground.tensorflow.org/#activation=relu&regularization=L1&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.003&noise=0&networkShape=4,2&seed=0.84485&showTestData=false&discretize=false&percTrainData=80&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a neural network with TensorFlow\n",
    "\n",
    "In this part of the tutorial, you will learn how to train a neural network with TensorFlow using the API's estimator DNNClassifier.\n",
    "\n",
    "We will use the MNIST dataset to train your first neural network. Training a neural network with Tensorflow is not very complicated. The preprocessing step looks precisely the same as in the previous tutorials. You will proceed as follow:\n",
    "\n",
    "- Step 1: Import the data\n",
    "- Step 2: Transform the data\n",
    "- Step 3: Construct the tensor\n",
    "- Step 4: Build the model\n",
    "- Step 5: Train and evaluate the model\n",
    "- Step 6: Improve the model\n",
    "\n",
    "### Step 1) Import the data\n",
    "\n",
    "First of all, you need to import the necessary library. You can import the MNIST dataset using scikit learn.\n",
    "\n",
    "The MNIST dataset is the commonly used dataset to test new techniques or algorithms. This dataset is a collection of 28x28 pixel image with a handwritten digit from 0 to 9. Currently, the lowest error on the test is 0.27 percent with a committee of 7 convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download scikit learn temporarily at this address. Copy and paste the dataset in a convenient folder. To import the data to python, you can use fetch_mldata from scikit learn. Paste the file path inside fetch_mldata to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "mnist_path = \"./mnist-original.mat\" #the MNIST file has been previously downloaded here\n",
    "mnist_raw = loadmat(mnist_path)\n",
    "mnist = {\n",
    "\"data\": mnist_raw[\"data\"].T,\n",
    "\"target\": mnist_raw[\"label\"][0],\n",
    "\"COL_NAMES\": [\"label\", \"data\"],\n",
    "\"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "}\n",
    "print(mnist['data'].shape)\n",
    "print(mnist['target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you import the data and get the shape of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784) (56000,) (14000, 784) (14000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist['data'], mnist['target'],\n",
    "                                                    test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "batch_size = len(X_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Transform the data\n",
    "\n",
    "In the previous tutorial, you learnt that you need to transform the data to limit the effect of outliers. In this tutorial, you will transform the data using the min-max scaler. The formula is: \n",
    "\n",
    "$$(x - min_x)/(max_x - min_x)$$\n",
    "\n",
    "Scikit learns has already a function for that: MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Train\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "# Test\n",
    "X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Construct the tensor\n",
    "\n",
    "You are now familiar with the way to create tensor in Tensorflow. You can convert the train set to a numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column('x', shape = X_train_scaled.shape[1:])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) Build the model\n",
    "\n",
    "The architecture of the neural network contains 2 hidden layers with 300 units for the first layer and 100 units for the second one. We use these value based on our own experience. You can tune theses values and see how it affects the accuracy of the network.\n",
    "\n",
    "To build the model, you use the estimator DNNClassifier. You can add the number of layers to the feature_columns arguments. You need to set the number of classes to 10 as there are ten classes in the training set. You are already familiar with the syntax of the estimator object. The arguments features columns, number of classes and model_dir are precisely the same as in the previous tutorial. The new argument hidden_unit controls for the number of layers and how many nodes to connect to the neural network. In the code below, there are two hidden layers with a first one connecting 300 nodes and the second one with 100 nodes.\n",
    "\n",
    "To build the estimator, use tf.estimator.DNNClassifier with the following parameters:\n",
    "\n",
    "- feature_columns: Define the columns to use in the network\n",
    "- hidden_units: Define the number of hidden neurons\n",
    "- n_classes: Define the number of classes to predict\n",
    "- model_dir: Define the path of TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/train/DNN', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015FAED4B1C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(feature_columns = feature_columns,\n",
    "                                      hidden_units = [300, 100],\n",
    "                                      n_classes = 10,\n",
    "                                      model_dir ='/train/DNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5) Train and evaluate the model\n",
    "\n",
    "You can use the numpy method to train the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /train/DNN\\model.ckpt-3000\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /train/DNN\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2196673, step = 3000\n",
      "INFO:tensorflow:global_step/sec: 368.332\n",
      "INFO:tensorflow:loss = 0.6404724, step = 3100 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 451.315\n",
      "INFO:tensorflow:loss = 1.5467565, step = 3200 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.62\n",
      "INFO:tensorflow:loss = 4.189549, step = 3300 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 516.848\n",
      "INFO:tensorflow:loss = 10.635938, step = 3400 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.344\n",
      "INFO:tensorflow:loss = 2.8429453, step = 3500 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 539.073\n",
      "INFO:tensorflow:loss = 0.84725523, step = 3600 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 547.908\n",
      "INFO:tensorflow:loss = 4.543673, step = 3700 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.28\n",
      "INFO:tensorflow:loss = 4.0514646, step = 3800 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 541.985\n",
      "INFO:tensorflow:loss = 0.4696637, step = 3900 (0.185 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /train/DNN\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.33646348.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-28T18:52:10Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /train/DNN\\model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-28-18:52:10\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.97407144, average_loss = 0.088940255, global_step = 4000, loss = 1245.1636\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /train/DNN\\model.ckpt-4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.97407144,\n",
       " 'average_loss': 0.088940255,\n",
       " 'loss': 1245.1636,\n",
       " 'global_step': 4000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the estimator\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(x = {\"x\": X_train_scaled},\n",
    "                                                y = y_train,\n",
    "                                                batch_size =50,\n",
    "                                                num_epochs = None,\n",
    "                                                shuffle = False)\n",
    "estimator.train(input_fn = train_input, steps = 1000)\n",
    "\n",
    "eval_input = tf.estimator.inputs.numpy_input_fn(x = {\"x\": X_test_scaled},\n",
    "                                               y = y_test,\n",
    "                                               batch_size = X_test_scaled.shape[0],\n",
    "                                               num_epochs = 1,\n",
    "                                               shuffle = False)\n",
    "estimator.evaluate(input_fn = eval_input, steps = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current architecture leads to an accuracy on the the evaluation set of 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6) Improve the model\n",
    "\n",
    "You can try to improve the model by adding regularization parameters.\n",
    "\n",
    "We will use an Adam optimizer with a dropout rate of 0.3, L1 of X and L2 of y. In TensorFlow, you can control the optimizer using the object train following by the name of the optimizer. TensorFlow is a built-in API for Proximal AdaGrad optimizer.\n",
    "\n",
    "To add regularization to the deep neural network, you can use tf.train.ProximalAdagradOptimizer with the following parameter\n",
    "\n",
    "- Learning rate: learning_rate\n",
    "- L1 regularization: l1_regularization_strength\n",
    "- L2 regularization: l2_regularization_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/train/DNN1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015FB4B4A908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /train/DNN1\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /train/DNN1\\model.ckpt.\n",
      "INFO:tensorflow:loss = 11.84285, step = 3000\n",
      "INFO:tensorflow:global_step/sec: 297.077\n",
      "INFO:tensorflow:loss = 6.666298, step = 3100 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.661\n",
      "INFO:tensorflow:loss = 10.621258, step = 3200 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.784\n",
      "INFO:tensorflow:loss = 19.399982, step = 3300 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.862\n",
      "INFO:tensorflow:loss = 23.858774, step = 3400 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.607\n",
      "INFO:tensorflow:loss = 11.480861, step = 3500 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 399.473\n",
      "INFO:tensorflow:loss = 7.4766293, step = 3600 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.047\n",
      "INFO:tensorflow:loss = 11.521801, step = 3700 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.046\n",
      "INFO:tensorflow:loss = 16.486107, step = 3800 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.282\n",
      "INFO:tensorflow:loss = 8.928833, step = 3900 (0.243 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /train/DNN1\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.131471.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-28T18:52:14Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /train/DNN1\\model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-28-18:52:15\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.9557143, average_loss = 0.15776448, global_step = 4000, loss = 2208.7026\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /train/DNN1\\model.ckpt-4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9557143,\n",
       " 'average_loss': 0.15776448,\n",
       " 'loss': 2208.7026,\n",
       " 'global_step': 4000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_imp = tf.estimator.DNNClassifier(feature_columns = feature_columns,\n",
    "                                          hidden_units = [300, 100],\n",
    "                                          dropout = 0.3,\n",
    "                                          n_classes = 10,\n",
    "                                          optimizer = tf.train.ProximalAdagradOptimizer(\n",
    "                                          learning_rate = 0.01,\n",
    "                                          l1_regularization_strength = 0.01,\n",
    "                                          l2_regularization_strength = 0.01),\n",
    "                                           model_dir = '/train/DNN1')\n",
    "estimator_imp.train(input_fn = train_input, steps = 1000)\n",
    "estimator_imp.evaluate(input_fn = eval_input, steps = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values chosen to reduce the over fitting did not improve the model accuracy. Your first model had an accuracy of 96% while the model with L2 regularizer has an accuracy of 95%. You can try with different values and see how it impacts the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learn how to build a neural network. A neural network requires:\n",
    "\n",
    "- Number of hidden layers\n",
    "- Number of fully connected node\n",
    "- Activation function\n",
    "- Optimizer\n",
    "- Number of classes\n",
    "\n",
    "In TensorFlow, you can train a neural network for classification problem with:\n",
    "\n",
    "- tf.estimator.DNNClassifier\n",
    "\n",
    "The estimator requires to specify:\n",
    "\n",
    "- feature_columns=feature_columns,\n",
    "- hidden_units=[300, 100]\n",
    "- n_classes=10\n",
    "- model_dir\n",
    "\n",
    "You can improve the model by using different optimizers. In this tutorial, you learned how to use Adam Grad optimizer with a learning rate and add a control to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
